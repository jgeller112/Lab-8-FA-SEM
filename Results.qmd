---
title: "Results"
format: html
editor: visual
---

# Method

## Data Preparation

First, the data were screened to determine their suitability for factor analyses. For our first test, we created a Pearson correlation matrix of the items of interest and measured the determinant of the matrix. We observed a number of correlations between the items. Most were weakly negative or positive (*R*s\<= +/- .33), while some were moderate and positive (*R*s = .33 to .67). One item ("Standard deviations excite me") was moderately negatively correlated with many of the other items (*R* between -.67 and -.33). The determinant of the correlation matrix was .0004, which is above the threshold of .00001 generally used to assess multicolinearity. As a result, we concluded that multicolinearty was not an issue in our data.

The second test of suitability which we ran was Bartlett's test of sphericity (Bartlett, 1950). The results of which suggested that the observed correlations in our matrix were significantly different from those under the null (i.e., an identity matrix): $\chi^2$ (253) = 9,534.27, *p* \< .001. This result suggests that there is enough correlation in the data to warrant a factor analysis.

We also conducted the Kaiser-Meyer-Olkin measure of sampling adequacy (KMO: Kaiser, 1970). KMO represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO can range from 0.00 to 1.00, with values closer to 1.00 indicating that the observed patterns of correlations are relatively compact and that factor analysis should yield distinct and reliable factors (Field, 2012). The overall KMO value in our dataset was .93, suggesting the data is highly suitable for factor analysis. All items in our data had a KMO \>= .79. As such, no items were excluded from our analyses.

Based on a composite outlier score (Lüdecke et al., 2021) obtained via the joint application of multiple outliers detection algorithms (Z-scores, Iglewicz, 1993; Interquartile range (IQR); Mahalanobis distance, Cabana, 2019; Robust Mahalanobis distance, Gnanadesikan and Kettenring, 1972; Minimum Covariance Determinant, Leys et al., 2018; Invariant Coordinate Selection, Archimbaud et al., 2018; OPTICS, Ankerst et al., 1999; Isolation Forest, Liu et al. 2008; and Local Outlier Factor, Breunig et al., 2000), we excluded 97 cases that were classified as outliers by at least half of the methods used.

There were no missing values in the data.

## Factor Extraction

Several criteria were used to determine the number of factors to extract: a parallel plot, a scree test, the eigenvalue-greater-than-one and greater-than-.77 criteria, and a consensus method relying on multiple tests.

Kaiser\'s eigenvalue-greater-than-one criteria suggested 1 factor while 2 factors were suggested by the eigenvalue-greater-than-.77 criteria. The inflexion (elbow) in the scree plot justified retaining 6 factors which the parallel plot also justified.

We also used a consensus method using the R package parameters (Lüdecke et al., 2020). This method uses many existing produces for determining the number of factors to extract. It returns a final number of factors to extract which is based on the maximum consensus between the different procedures. This method suggested a four-factor solution, which more than 30% of the procedures tested recommended.

Although we note the divergence among the suggested number of factors, we ultimately choose to proceed with the recommendation of the consensus method. It's a more powerful estimation of the appropriate number of factors because it relies on information across a great many different tests. As such, we extracted four factors using principal axis factoring and oblique rotation.

All analyses were conducted using the R statistical programming language (version 4.2.2; R Core Team, 2022)

# Results

The results of the exploratory factor analysis are displayed in **Table 1**. The recovered four-factor structure suggested that the following components are involved in students' statistics anxiety: general statistics anxiety ("I don't understand statistics"; "statistics makes me cry"); technology anxiety or anxiety centered on the tools used to conduct statistical analyses ("R always crashes when I try to use it"; "computers are out to get me"); math anxiety ("I slip into a coma whenever I see an equation"; "I have never been good at mathematics"); and social anxiety or fear of social comparison in the domain of statistics ("my friends are better at statistics than me"; "everybody looks at me when I use R"). We also present the factor loadings visually in **Figure 1**.

**Table 1**

*Item Factor Loadings and Communality, Uniqueness, and Complexity Scores*

| Factor             | Item | Description                                                                            | Statistics Anxiety   | Technology Anxiety  | Math Anxiety         | Social anxiety       | Communality        | Uniqueness         | Complexity         |
|----------|--------|------------------|-------------|--------------|----------|-----------|-----------|----------|----------|
| Statistics anxiety | 21   | I wake up under my duvet thinking that I am trapped under a normal distribution        | 0.612                | 0.050               | 0.032                | -0.081               | 0.48               | 0.52               | 1.05               |
|                    | 4    | I dream that Pearson is attacking me with correlation coefficients                     | 0.531                | 0.087               | 0.136                | 0.044                | 0.42               | 0.58               | 1.20               |
|                    | 1    | Statistics makes me cry                                                                | 0.515                | 0.028               | 0.192                | 0.089                | 0.39               | 0.61               | 1.34               |
|                    | 16   | I weep openly at the mention of central tendency                                       | 0.490                | 0.083               | 0.155                | -0.083               | 0.46               | 0.54               | 1.32               |
|                    | 20   | I can't sleep for thoughts of eigenvectors                                             | 0.480                | -0.164              | 0.041                | -0.184               | 0.28               | 0.72               | 1.56               |
|                    | 12   | People try to tell you that R makes statistics easier to understand but it doesn't     | 0.479                | 0.262               | -0.029               | -0.088               | 0.46               | 0.54               | 1.64               |
|                    | 5    | I don't understand statistics                                                          | 0.455                | 0.105               | 0.090                | 0.029                | 0.32               | 0.68               | 1.20               |
|                    | 3    | Standard deviations excite me                                                          | [-0.403]{.underline} | [0.011]{.underline} | [-0.084]{.underline} | [0.377]{.underline}  | [0.48]{.underline} | [0.52]{.underline} | [2.08]{.underline} |
| Technology anxiety | 6    | I have little experience of computers                                                  | -0.121               | 0.823               | 0.024                | -0.006               | 0.61               | 0.39               | 1.04               |
|                    | 18   | R always crashes when I try to use it                                                  | 0.278                | 0.561               | 0.005                | -0.031               | 0.57               | 0.43               | 1.47               |
|                    | 7    | I slip into a coma whenever I see an equation                                          | 0.276                | 0.501               | 0.038                | -0.016               | 0.51               | 0.49               | 1.57               |
|                    | 13   | I worry that I will cause irreparable damage because of my incompetence with computers | 0.152                | 0.491               | 0.142                | -0.068               | 0.49               | 0.51               | 1.41               |
|                    | 14   | Computers have minds of their own and deliberately go wrong whenever I use them        | [0.317]{.underline}  | [0.381]{.underline} | [0.037]{.underline}  | [-0.051]{.underline} | [0.42]{.underline} | [0.58]{.underline} | [2.00]{.underline} |
|                    | 10   | Computers are useful only for playing games                                            | 0.061                | 0.354               | 0.060                | -0.052               | 0.20               | 0.80               | 1.16               |
|                    | 15   | Computers are out to get me                                                            | [0.142]{.underline}  | [0.280]{.underline} | [0.198]{.underline}  | [-0.140]{.underline} | [0.33]{.underline} | [0.67]{.underline} | [2.92]{.underline} |
| Math anxiety       | 8    | I have never been good at mathematics                                                  | 0.000                | -0.069              | 0.856                | 0.062                | 0.67               | 0.33               | 1.02               |
|                    | 11   | I did badly at mathematics ar school                                                   | -0.033               | 0.072               | 0.761                | -0.097               | 0.65               | 0.35               | 1.05               |
|                    | 17   | I slip into a coma whenever I see an equation                                          | 0.083                | 0.060               | 0.690                | 0.013                | 0.59               | 0.41               | 1.04               |
| Social anxiety     | 9    | My friends are better at statistics than me                                            | 0.019                | -0.022              | 0.060                | 0.611                | 0.36               | 0.64               | 1.02               |
|                    | 2    | My friends will think that I'm stupid for not being able to cope with R                | -0.088               | 0.066               | 0.025                | 0.532                | 0.30               | 0.70               | 1.09               |
|                    | 22   | My friends are better at R than I am                                                   | 0.148                | -0.120              | -0.095               | 0.483                | 0.26               | 0.74               | 1.41               |
|                    | 19   | Everybody looks at me when I use R                                                     | -0.174               | -0.055              | -0.031               | 0.366                | 0.26               | 0.74               | 1.50               |
|                    | 23   | If I'm good at statistics my friends will think I'm a nerd                             | 0.151                | -0.005              | -0.110               | 0.337                | 0.11               | 0.89               | 1.63               |

*Note*. Items with [underlined]{.underline} factor loadings loaded strongly onto multiple factors (i.e., were cross-loaded)

**Figure 1**

*Factor Loadings*

![](efa_fact_plot.png){width="762"}

*Note*. The descriptions corresponding to the items listed on the y-axis can be read in **Table 1**.

To assess the goodness of fit of our model, we ran a number of tests. The results of a chi-square test suggested that the model did not fit the data well: $\chi^2$ (246) = 945.09, *p* \< .001. This test is sensitive to sample size however, and with larger samples it is more likely to detect even small discrepancies between the model and the data. This might not be the best indicator of fit for our model and data as a result (*N* = 742). We also computed the comparative fit index (CFI) as .88. As a general guideline, a CFI value close to or greater than .95 indicates a good fit (Hu & Bentler, 1999). Our model's CFI is below that threshold, suggesting a sub-optimal fit to the data. We also calculated the root mean square error of approximation (RMSEA) and the standardized root mean square residual (SRMR): RMSEA = .062, 95% CIs = \[.06, .07\]; SRMR = 0.05. RMSEA values between .05 and .08 indicate a reasonable model fit (Browne & Cudeck, 1993) and so we can say that our model fit the data reasonably well on this metric. A SRMR of .08 or less is generally considered to indicate good fit (Hu & Bentler, 1999). Thus, our model also displays a good fit of the data on this metric.

The different model fit indices could lead to conflicting conclusions. On the one hand, the $\chi^2$ test and CFI suggest a relatively poor fit. While the RMSEA and SRMR indicate a reasonable or even good fit. Given the limitations with the $\chi^2$ test in terms of sample size, we ought to weight its result less than the others as we judge model fit. We are then left with two metrics indicating acceptable fit and one that suggests sub-optimal fit. Taking the consensus among the metrics would leads us to conclude that the model fits the data at least reasonably well.
